{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMcB5lsu/BP3hWy13KC3OT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/ai_programming_2022/blob/main/07_advanced_ai/03_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxam5J6s8Pw9"
      },
      "source": [
        "# 演習: 深層強化学習のテクニック\n",
        "学習を安定化させるためのテクニックを実装しましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zQz9K3nzGON"
      },
      "source": [
        "## 深層強化学習のテクニック\n",
        "深層強化学習では、一般的に以下のようなテクニックが利用されます。\n",
        "\n",
        "### Experience Replay\n",
        "各ステップの内容をメモリに保存しておき、メモリからランダムに記録を取り出して学習を行います。  \n",
        "これにより、ミニバッチ法を使用することが可能になります。  \n",
        "また、ミニバッチに時間的にばらけた記録が入ることになるので、学習が安定します。  \n",
        "\n",
        "### Fixed Target Q-Network\n",
        "行動を決定するのに用いるmain-networkと、誤差の計算時に正解を求めるのに用いるtarget-networkを用意します。  \n",
        "target-networkのパラメータは一定時間固定されますが、main-networkのパラメータはミニバッチごとに更新されます。そして、定期的にmain-networkのパラメータがtarget-networkに上書きされます。これにより、正解が短い時間で揺れ動く問題が低減され学習が安定すると考えられています。  \n",
        "  \n",
        "今回の演習では、このうちのFixed Target Q-Networkを実装します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2iCWKom81u5"
      },
      "source": [
        "## ライブラリの導入"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfOUkvmVAmmY"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt \n",
        "from matplotlib import animation, rc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mjp80sVK1kue"
      },
      "source": [
        "## Netクラス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpiskMvFmhAq"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, n_state, n_mid, n_action):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(n_state, n_mid)  # 全結合層\n",
        "        self.fc2 = nn.Linear(n_mid, n_mid)\n",
        "        self.fc3 = nn.Linear(n_mid, n_action)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAGWByiE9Q1M"
      },
      "source": [
        "## Brainクラス\n",
        "以下のセルのコードを補完し、Fixed Target Q-Networkを実装しましょう。  \n",
        "main-network、target-networkの2つのネットワークを用意します。  \n",
        "正解を用意するためにはtarget-networkを使用しますが、target-networkには一定間隔でmain-networkのパラメータをコピーします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7mzk3ujGwjY"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, n_state, n_action, main_net, target_net,\n",
        "                 loss_fnc, optimizer, target_interval, is_gpu, gamma=0.9, r=0.99, lr=0.01):\n",
        "        self.n_state = n_state  # 状態の数\n",
        "        self.n_action = n_action  # 行動の数\n",
        "\n",
        "        self.main_net = main_net  # main-network\n",
        "        self.target_net = target_net  # target-network\n",
        "        self.loss_fnc = loss_fnc  # 誤差関数\n",
        "        self.optimizer = optimizer  # 最適化アルゴリズム\n",
        "\n",
        "        self.target_interval = target_interval\n",
        "        self.target_count = 0\n",
        "\n",
        "        self.is_gpu = is_gpu  # GPUを使うかどうか\n",
        "        if self.is_gpu:\n",
        "            self.main_net.cuda()  # GPU対応\n",
        "            self.target_net.cuda()  # GPU対応\n",
        "\n",
        "        self.eps = 1.0  # ε\n",
        "        self.gamma = gamma  # 割引率\n",
        "        self.r = r  # εの減衰率\n",
        "        self.lr = lr  # 学習係数\n",
        "\n",
        "    def train(self, states, next_states, action, reward, terminal):  # ニューラルネットワークを訓練\n",
        "\n",
        "        states = torch.from_numpy(states).float()\n",
        "        next_states = torch.from_numpy(next_states).float()\n",
        "        if self.is_gpu:\n",
        "            states, next_states = states.cuda(), next_states.cuda()  # GPU対応\n",
        "            \n",
        "        self.target_net.eval()  # 評価モード\n",
        "        next_q =   # ------------- この行にコードを追記 -------------\n",
        "        self.main_net.train()  # 訓練モード\n",
        "        q =   # ------------- この行にコードを追記 -------------\n",
        "\n",
        "        t = q.clone().detach()\n",
        "        if terminal:\n",
        "            t[:, action] = reward  #  エピソード終了時の正解は、報酬のみ\n",
        "        else:\n",
        "            t[:, action] = reward + self.gamma*np.max(next_q.detach().cpu().numpy(), axis=1)[0]\n",
        "            \n",
        "        loss = self.loss_fnc(q, t)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.target_count += 1\n",
        "        if self.target_count >= self.target_interval:\n",
        "            self.target_net.load_state_dict(self.main_net.state_dict())\n",
        "            self.target_count = 0\n",
        "\n",
        "    def get_action(self, states):  # 行動を取得\n",
        "        states = torch.from_numpy(states).float()\n",
        "        if self.is_gpu:\n",
        "            states = states.cuda()  # GPU対応\n",
        "\n",
        "        if np.random.rand() < self.eps:  # ランダムな行動\n",
        "            action = np.random.randint(self.n_action)\n",
        "        else:  # Q値の高い行動を選択\n",
        "            q = self.main_net.forward(states)\n",
        "            action = np.argmax(q.detach().cpu().numpy(), axis=1)[0]\n",
        "        if self.eps > 0.1:  # εの下限\n",
        "            self.eps *= self.r\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9t6fuQvDPOf"
      },
      "source": [
        "## エージェントのクラス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DP4AaA5bHTCQ"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, v_x, v_y_sigma, v_jump, brain):\n",
        "        self.v_x = v_x  # x速度\n",
        "        self.v_y_sigma = v_y_sigma  # y速度、初期値の標準偏差\n",
        "        self.v_jump = v_jump  # ジャンプ速度\n",
        "        self.brain = brain\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.x = -1  # 初期x座標\n",
        "        self.y = 0  # 初期y座標\n",
        "        self.v_y = self.v_y_sigma * np.random.randn()  # 初期y速度\n",
        "\n",
        "    def step(self, g):  # 時間を1つ進める g:重力加速度\n",
        "        states = np.array([[self.y, self.v_y]])\n",
        "        self.x += self.v_x\n",
        "        self.y += self.v_y\n",
        "\n",
        "        reward = 0  # 報酬\n",
        "        terminal = False  # 終了判定\n",
        "        if self.x>1.0:\n",
        "            reward = 1\n",
        "            terminal = True\n",
        "        elif self.y<-1.0 or self.y>1.0:\n",
        "            reward = -1\n",
        "            terminal = True\n",
        "\n",
        "        action = self.brain.get_action(states)\n",
        "        if action == 0:\n",
        "            self.v_y -= g   # 自由落下\n",
        "        else:\n",
        "            self.v_y = self.v_jump  # ジャンプ\n",
        "        next_states = np.array([[self.y, self.v_y]])\n",
        "        self.brain.train(states, next_states, action, reward, terminal)\n",
        "\n",
        "        if terminal:\n",
        "            self.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr-bkx6REXCx"
      },
      "source": [
        "## 環境のクラス"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqCxzW2RHYrl"
      },
      "source": [
        "class Environment:\n",
        "    def __init__(self, agent, g):\n",
        "        self.agent = agent\n",
        "        self.g = g  # 重力加速度\n",
        "\n",
        "    def step(self):\n",
        "        self.agent.step(self.g)\n",
        "        return (self.agent.x, self.agent.y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hot23xovE-fj"
      },
      "source": [
        "## アニメーション"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuEBjtK-JNbz"
      },
      "source": [
        "def animate(environment, interval, frames):\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.close()\n",
        "    ax.set_xlim(( -1, 1))\n",
        "    ax.set_ylim((-1, 1))\n",
        "    sc = ax.scatter([], [])\n",
        "\n",
        "    def plot(data):\n",
        "        x, y = environment.step()\n",
        "        sc.set_offsets(np.array([[x, y]]))\n",
        "        return (sc,)\n",
        "\n",
        "    return animation.FuncAnimation(fig, plot, interval=interval, frames=frames, blit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XhwfFz1GKOl"
      },
      "source": [
        "## 深層強化学習の実行\n",
        "2つのニューラルネットワーク、Brain、エージェント、環境の設定を行い、深層強化学習を実行します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOryPVWuJTme"
      },
      "source": [
        "n_state = 2\n",
        "n_mid = 32\n",
        "n_action = 2\n",
        "\n",
        "main_net = Net(n_state, n_mid, n_action)\n",
        "target_net = Net(n_state, n_mid, n_action)\n",
        "target_net.load_state_dict(main_net.state_dict())\n",
        "target_net.eval()\n",
        "\n",
        "loss_fnc = nn.MSELoss()  # 誤差関数\n",
        "optimizer = optim.RMSprop(main_net.parameters(), lr=0.01)  # 最適化アルゴリズム\n",
        "target_interval = 10\n",
        "is_gpu = True\n",
        "\n",
        "brain = Brain(n_state, n_action, main_net, target_net,\n",
        "              loss_fnc, optimizer, target_interval, is_gpu) \n",
        "\n",
        "v_x = 0.05\n",
        "v_y_sigma = 0.1\n",
        "v_jump = 0.2\n",
        "agent = Agent(v_x, v_y_sigma, v_jump, brain)\n",
        "\n",
        "g = 0.2\n",
        "environment = Environment(agent, g)\n",
        "\n",
        "anim = animate(environment, 50, 1024)\n",
        "rc(\"animation\", html=\"jshtml\")\n",
        "anim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVn-iSjcGoco"
      },
      "source": [
        "# 解答例\n",
        "以下に解答例を掲載します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiDlW_vG1MdI"
      },
      "source": [
        "class Brain:\n",
        "    def __init__(self, n_state, n_action, main_net, target_net,\n",
        "                 loss_fnc, optimizer, target_interval, is_gpu, gamma=0.9, r=0.99, lr=0.01):\n",
        "        self.n_state = n_state  # 状態の数\n",
        "        self.n_action = n_action  # 行動の数\n",
        "\n",
        "        self.main_net = main_net  # main-network\n",
        "        self.target_net = target_net  # target-network\n",
        "        self.loss_fnc = loss_fnc  # 誤差関数\n",
        "        self.optimizer = optimizer  # 最適化アルゴリズム\n",
        "\n",
        "        self.target_interval = target_interval\n",
        "        self.target_count = 0\n",
        "\n",
        "        self.is_gpu = is_gpu  # GPUを使うかどうか\n",
        "        if self.is_gpu:\n",
        "            self.main_net.cuda()  # GPU対応\n",
        "            self.target_net.cuda()  # GPU対応\n",
        "\n",
        "        self.eps = 1.0  # ε\n",
        "        self.gamma = gamma  # 割引率\n",
        "        self.r = r  # εの減衰率\n",
        "        self.lr = lr  # 学習係数\n",
        "\n",
        "    def train(self, states, next_states, action, reward, terminal):  # ニューラルネットワークを訓練\n",
        "\n",
        "        states = torch.from_numpy(states).float()\n",
        "        next_states = torch.from_numpy(next_states).float()\n",
        "        if self.is_gpu:\n",
        "            states, next_states = states.cuda(), next_states.cuda()  # GPU対応\n",
        "            \n",
        "        self.target_net.eval()  # 評価モード\n",
        "        next_q = self.target_net.forward(next_states)  # ------------- この行にコードを追記 -------------\n",
        "        self.main_net.train()  # 訓練モード\n",
        "        q = self.main_net.forward(states)  # ------------- この行にコードを追記 -------------\n",
        "\n",
        "        t = q.clone().detach()\n",
        "        if terminal:\n",
        "            t[:, action] = reward  #  エピソード終了時の正解は、報酬のみ\n",
        "        else:\n",
        "            t[:, action] = reward + self.gamma*np.max(next_q.detach().cpu().numpy(), axis=1)[0]\n",
        "            \n",
        "        loss = self.loss_fnc(q, t)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.target_count += 1\n",
        "        if self.target_count >= self.target_interval:\n",
        "            self.target_net.load_state_dict(self.main_net.state_dict())\n",
        "            self.target_count = 0\n",
        "\n",
        "    def get_action(self, states):  # 行動を取得\n",
        "        states = torch.from_numpy(states).float()\n",
        "        if self.is_gpu:\n",
        "            states = states.cuda()  # GPU対応\n",
        "\n",
        "        if np.random.rand() < self.eps:  # ランダムな行動\n",
        "            action = np.random.randint(self.n_action)\n",
        "        else:  # Q値の高い行動を選択\n",
        "            q = self.main_net.forward(states)\n",
        "            action = np.argmax(q.detach().cpu().numpy(), axis=1)[0]\n",
        "        if self.eps > 0.1:  # εの下限\n",
        "            self.eps *= self.r\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}